---
title:  "[논문 리뷰] 두 번째 읽은 BERT"
excerpt: "BERT를 다시 읽고 중요한 내용만 정리했습니다"

categories:
  - NLP
tags:
  - [DL]

toc: true
toc_sticky: true
---

# BERT

* GPT와 다른 양방향 언어 모델, MLM 기법 사용, NSP 사용

## MLM

* MLM - input sequence의 15% wordpiece token을 mask 처리함.

-> MLM의 단점으로 pre-training과 fine-tuning간 MASK token으로 인한 단어사전 불일치 문제를 언급함.   

-> 불일치 문제를 완화하기 위해 15% 전부를 MASK하지 않음.   

-> 15%중 80%를 MASK token으로 대체, 10%는 다른 단어로 대체, 10%는 변화시키지 않음.

-> 그리고 이 15% 전체에 대해 cross-entropy loss를 사용해 원래 단어가 무엇인지 예측

## NSP

* QA나 NLI task를 위해 문장간 관계를 학습하는 NSP 사용

-> 문장 A와 B를 고를 때 50% 확률로 B를 A의 진짜 다음 문장인 B를 고르거나 실제로 이어지지 않는 문장 B를 고름.   
-> sequence의 CLS token을 NSP 분류할 때 사용함.

## Ablation Study

-> NSP를 제거하니 QNLI, MNLI, SQuAD 1.1과 같은 task의 성능이 떨어짐.   
-> No NSP와 LTR & No NSP를 비교했을 때 MLM을 사용한 모델보다 LTR 모델을 사용했을 때 성능이 떨어짐 특히 MRPC와 SQuAD task의 성능이 크게 떨어짐.