---
title:  "[논문 리뷰] 두 번째 읽는 ELECTRA 논문"
excerpt: "첫 번째는 해석을 한 것 같아 두 번째 읽고 중요한 내용을 정리했습니다"

categories:
  - NLP
tags:
  - [DL]

toc: true
toc_sticky: true
---

# ELECTRA

## Replaced Token Detection

* Generator와 Discriminator로 나뉘어지며 Discriminator가 ELECTRA임.

* GAN의 구조와 비슷함.   
* Generator는 MASK 토큰을 실제와 비슷하지만 다른 그럴듯한 단어로 대체하며 이를 통해 나온 output이 ELECTRA의 입력이 됨.   
* ELECTRA의 output을 통해 기존 문장과 각 token이 일치하는지 아닌지를 이진 분류를 함.   
* 이미지와 달리 실제와 일치하는 token을 만들었을 경우 fake가 아닌 real로 분류함.   
  -> 이미지는 비슷한 이미지가 픽셀까지 같지는 않음. 하지만 단어는 가짜로 만든 단어가 진짜일 때 이를 진짜인지 가짜인지 구분하지를 못 함.

## Weight Sharing

* Generator와 Discriminator의 크기가 동일하면 모든 가중치를 공유하지만 실험을 통해 Generator의 크기가 작은 것이 성능이 좋다는 것을 발견함.   
  -> 따라서 Generator와 Discriminator의 token과 positional 인코딩의 가중치만을 공유함.

* ELECTRA의 성능은 token 임베딩의 가중치를 공유했을 때 좋음.   
  -> 모든 가중치를 공유하면 성능이 더 좋아지지만 Generator와 Discriminator의 크기를 동일하게 설정해야 하는 단점과 비교하면 많은 성능 개선이 있지 않으므로 token embedding만을 공유함.

## Small Generator

* Generator와 Discriminator 두개를 사용하므로 연산량이 BERT와 비교하면 두배 가까이 들음. 따라서 Generator의 크기를 줄이는 실험을 진행함.

* Generator의 크기를 줄이면 성능이 향상됨.
  -> Generator의 성능이 좋으면 진짜와 같은 그럴듯한 문장을 만들어 Discriminator가 학습하기 어려움 (진짜 단어와 동일한 단어를 만들어 fake를 학습이 안됨.)

* Discriminator의 1/2 or 1/4 크기로 줄이는 것이 가장 효율적임.

## Train Algorithm

* 훈련시키는 방법은 총 3가지를 실험했다.   
  -> Generator와 Discriminator를 같이 학습.   
  -> 강화학습을 통한 적대적 학습 방법.   
  -> two-stage을 통한 학습 방법.

* 기존 ELECTRA의 학습 방법인 같이 학습하는 것이 가장 좋은 성능을 보였지만 다른 방법들도 기존 BERT보다는 좋은 성능을 보임.

## Small Models

* ELECTRA의 목표는 BERT를 단일 GPU로 빠르게 사전학습 시키는 것을 목표로 함.   
  -> sequence의 길이를 512에서 128로 줄임.

  -> batch size를 256에서 128로 줄임.

  -> hidden dimension size를 756에서 256로 줄임.

  -> token embedding의 크기를 756에서 128로 줄임.

## Effiency Analysis

* ELECTRA의 높은 성능이 어떻게 나오는지 조금 더 자세하게 이해하기 위해 몇 가지 실험을 진행함  

* ELECTRA 15% : ELECTRA와 동일하지만 loss를 mask된 단어로만 구함.

* Replace MLM : MLM이랑 똑같지만 pre-training과 fine-tuning간 불일치를 해결하기 위해 [MASK] token 대신 generator가 만든 단어를 사용.

* All-Tokens MLM : Replace MLM과 동일하지만 MASK된 token만을 예측하는 것이 아닌 모든 단어를 MLM 방식으로 예측을 함.   

  -> ELECTRA 15%는 ELECTRA보다 성능이 좋지 않음.   

  -> Replace MLM이 BERT보다 성능이 높았던 것을 보아 mismatch로 인해 성능이 하락한다는 것을 발견함.   

  -> All-Tokens MLM은 BERT와 ELECTRA 사이 성능을 보임.

  -> 따라서 ELECTRA의 향상은 불일치 문제를 해결하고 사전학습을 할 때 모든 token을 사용하면서 기여됨.