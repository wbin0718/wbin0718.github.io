---
title:  "[논문 리뷰] 두 번째 읽는 Roberta"
excerpt: "첫 번째 Roberta 논문을 읽었을 때는 해석을 한 것 같아서 다시 읽고 중요한 내용을 정리했습니다."

categories:
  - NLP
tags:
  - [DL]

toc: true
toc_sticky: true
---

# RoBERTa

-> 등장 배경?   
-> BERT가 과소 적합 되었다고 주장함.   
-> BERT를 향상된 방법으로 학습하는 방법을 제한하기 위해 나옴.

* 수정된 내용은 총 4가지가 있음.   

1. 더 많은 데이터와 더 큰 batch size로 더욱 길게 학습 함.
2. NSP를 제거.
3. 더욱 긴 input sequence를 사용.
4. input sequence에 적용되는 masking pattern을 다이나믹하게 바꿈.

## Stactic Masking VS Dynamic Masking

* bert는 데이터 전처리 할 때 masking 한번만을 하므로 매 epoch마다 같은 masking된 데이터를 봄.   
* RoBERTa는 데이터를 복사를 해서 각기 다른 masking이 적용되도록 설정함. 이를 통해 매 epoch마다 같은 데이터를 보는 횟수를 감소시킴.

-> RoBERTa를 stactic과 dynamic masking 방법을 사용했을 때와 BERT를 비교했을 때 stactic RoBERTa는 비슷한 성능을 보였고, dynamic RoBERTa는 더 높은 성능을 보였다.

## NSP

* BERT는 NSP가 일부 task의 성능을 하락시킨다고 했지만 다른 연구들은 NSP가 필요한지 의문을 제기한다고 함. 따라서 몇가지 실험을 진행함.

* Segment-Pair + NSP   
  -> 버트와 동일한 구조
   
* Sentence-Pair + NSP   
  -> 인접한 문장 쌍으로 segment를 구성 (기존 bert와 차이점은 각 segment가 여러 문장의 token을 가지고 있는 반면 Sentence-Pair는 두 문장만이 사용되는 것 같음)   
  -> 하나의 문장만을 사용하므로 장기-의존성 문제로 성능이 하락함.
   
* Full-Sentences   
  -> 한 document의 모든 문장으로 최대길이를 채우고 다음 document로 넘어갈 때 SEP로 구분함.
   
* Doc-Sentences   
  -> Full-Sentences와 동일하지만 다음 document로 넘어가면 채우지 않음.   

   
* NSP를 제거한 Full-Sentences와 Doc-Sentences가 NSP를 사용한 Segment-Pair보다 높은 성능을 보였음. 따라서 NSP는 없어도 된다고 주장.   
  -> Doc-Sentences가 Full-Sentences보다 좋은 성능을 보이지만 Batch Size가 변하는 문제로 Full-Sentences 사용함.

## With Larger Batch Size

* 배치 사이즈를 증가시켰을 때 Perplexity가 낮아짐.

## Text Encoding

* 기존 BERT는 30K character-level BPE를 사용한 반면 RoBERTa는 50K byte-level BPE를 사용함.
* RoBERTa는 언급한 것 이외에도 더 많은 데이터로 학습을 진행하고 학습 시간을 증가시키는 차이가 있다.