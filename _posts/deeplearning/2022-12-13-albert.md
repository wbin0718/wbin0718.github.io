---
title:  "[논문 리뷰] ALBERT"
excerpt: "ALBERT 논문을 읽고 중요한 내용을 정리했습니다"

categories:
  - NLP
tags:
  - [DL]

toc: true
toc_sticky: true
---

# ALBERT

* 두가지의 파라미터 줄이는 기법을 소개함.

## Factorized embedding parameterazation

* 단어사전의 크기는 V가 되고 hidden layer의 크기는 H가 되는데 이 값이 커지면 V*E 역시 증가해서 파라미터의 개수가 증가한다.

* 따라서 E의 크기를 줄인 다음 작은 E 크기로 projection하고 이를 다시 hidden layer의 크기로 projection하는 기법을 사용.

  -> embedding 크기가 hidden layer의 크기보다 작을 때 효과적이다.

## Cross-layer parameter sharing

* 피드포워드 layer의 파라미터를 공유하거나 어텐션 layer의 파라미터를 공유하는 방법이 있다. ALBERT는 기본적으로 모든 파라미터를 공유한다.

## Inter-sentence coherence loss

* NSP는 MLM과 비교하면 어려운 task가 아니므로 효과적이지 않다.

* SOP는 NSP와 비슷하게 연속된 segments를 사용하지만 두 segmet의 순서를 바꾼다.

* SOP는 NSP task를 해결하지만 NSP는 SOP task를 해결하지 못 한다.

